import pandas as pd
import subprocess
import os
from collections import defaultdict

configfile: 'project_config_file.yaml'
localrules: aggregate_fastqc_raw, aggregate_trim_stderr, aggregate_RSeQC

config = defaultdict(str, config)

#### Assume demultiplexing was done with bcl2fastq2 with specified adapters in sample sheet.
demult = True
if config['demultiplex'] == 'SSSS':
  demult = False ### Unless 'SSSS' is input, meaning Sam's sans-sample-sheet demultiplexing code
if config['adapter'] == 'Illumina':
  adapter_file = '/hpcdata/vrc/vrc1_data/douek_lab/programs/Trimmomatic-0.39/adapters/TruSeq3-PE.fa'
elif config['adapter'] == 'Nextera':
  adapter_file = '/hpcdata/vrc/vrc1_data/douek_lab/programs/Trimmomatic-0.39/adapters/NexteraPE-PE.fa'

do_annot = False
if(config['annotation_reference'] == ''):
  do_annot = True

cell_sheet = os.path.join(os.getcwd(), config['Cell_sheet'])
cells = pd.read_table(cell_sheet, sep = ',').set_index("Cell_ID", drop = False)
print('Size of input sample sheet')
print(cells.shape)

cells['Lane'] = cells['Lane'].apply(str)

### List of test variable names from the configuration file
tests = [x.strip() for x in config['test'].split(',')]
print('Tests:')
print(tests)
### Define stratifications
#strats = [x.strip() for x in config['stratifications'].split(':')]
#strat_list = ['All']
#if(len(strats) > 1):
#  strat_list = strat_list + [strats[0] + '-' + v.strip() for v in strats[1].split(',')]

if 'clust_resolution2' not in config.keys():
  config['clust_resolution2'] = config['cluster_resolution']

### Read list of clusters to sub-cluster
sc_file = os.path.join(os.getcwd(), config['sub_cluster'])
if os.path.exists(sc_file):
  sc = pd.read_csv(sc_file)
  ### Because cluster names are likely numbers only
  sc['cluster'] = sc['cluster'].astype(str)
sub_clusters = ['-'.join(list(row)) for index,row in sc.iterrows()]
print('Sub clustering: ')
print(sub_clusters)

if demult: ### Requires Cell_ID, Cell_Name, S_N, Lane and flowcell_full
  ### fastq file names  output from bcl2fastq2
  cells['R1'] = cells['Cell_Name'].map(str) + '_' + cells['S_N'].map(str) + '_L00' + cells['Lane'].map(str) + '_R1_001' 
  cells['R2'] = cells['Cell_Name'].map(str) + '_' + cells['S_N'].map(str) + '_L00' + cells['Lane'].map(str) + '_R2_001' 
  ### fastq file paths  output from bcl2fastq (and then Sam's demultiplexing?)
  #cells['R1_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'bcl2fastq2', config['project'], cells['Cell_ID'][i], cells['R1'][i] + '.fastq.gz') for i in range(0, len(cells['R1']))])
  #cells['R2_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'bcl2fastq2', config['project'], cells['Cell_ID'][i], cells['R2'][i] + '.fastq.gz') for i in range(0,len(cells['R1']))])
  cells['R1_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', config['project'], cells['Cell_ID'][i], cells['R1'][i] + '.fastq.gz') for i in range(0, len(cells['R1']))])
  cells['R2_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', config['project'], cells['Cell_ID'][i], cells['R2'][i] + '.fastq.gz') for i in range(0,len(cells['R1']))])
else:  ### Requires UDP_ID and Lane and flowcell_full
  ### fastq file names  output from Sam's sans-sample-sheet code
  cells['R1'] = 'read1_index_' + cells['UDP_ID'].map(str) 
  cells['R2'] = 'read2_index_' + cells['UDP_ID'].map(str)
  #cells['R1'] = 'paired_read1_index_' + cells['UDP_ID'].map(str) 
  #cells['R2'] = 'paired_read2_index_' + cells['UDP_ID'].map(str)
  ### fastq file paths  output from bcl2fastq (and then Sam's demultiplexing?)
  cells['R1_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', cells['Lane'][i], cells['UDP_ID'][i], cells['R1'][i] + '.fq.gz') for i in range(0, len(cells['R1']))])
  cells['R2_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', cells['Lane'][i], cells['UDP_ID'][i], cells['R2'][i] + '.fq.gz') for i in range(0,len(cells['R1']))])

print('Before and after filtering for existing but empty fastq.gz files')
print(cells.shape)

### Remove cells with raw fastq files that do not exist
cells['filt'] = list([os.path.exists(x) for x in cells['R1_fullpath']])
print(str(sum(cells['filt'] == False)) + ' removed because fastq.gz do not exist')

cells = cells[cells['filt'] == True]
### Remove cells with raw fastq files that exist but are empty
cells['filt'] = list([(os.path.getsize(x) > 0) for x in cells['R1_fullpath']])
print(str(sum(cells['filt'] == False)) + ' removed because fastq.gz have no size')
cells = cells[cells['filt'] == True]

print(cells.shape)

### List of full paths to R1 and R2
runs_fastqs = list(cells['R1_fullpath']) + list(cells['R2_fullpath'])
runs_fastqs_key = list([cid + '_R1' for cid in cells['Cell_ID']]) + list([cid + '_R2' for cid in cells['Cell_ID']])
### Need dictionary Cell_ID -> R1 and Cell_ID -> R2

#cells['Cell_ID_R1'] = cells['Cell_ID'].map(str) + '_R1'
#cells['Cell_ID_R2'] = cells['Cell_ID'].map(str) + '_R2'
#fastq_names = list(cells['Cell_ID_R1']) + list(cells['Cell_ID_R2'])
raw_fastq_names = list(cells['R1']) + list(cells['R2'])


### Trimmomatic error logs per cell
cells['trimerr'] = list([os.path.join('data', 'fastq', 'trimmed', cells['Cell_ID'][i], 'trim_stderr.txt') for i in range(0,len(cells['R1']))])
trimerr_files = cells['trimerr']
### fastq file paths output from trimmomatic and moved to project directory. IS THIS CURRENTLY USED?
cells['R1_projpath'] = list([os.path.join('data', 'fastq', 'trimmed', cells['Cell_ID'][i], cells['Cell_ID'][i] + '_R1.fq.gz') for i in range(0,len(cells['R1']))])
cells['R2_projpath'] = list([os.path.join('data', 'fastq', 'trimmed', cells['Cell_ID'][i], cells['Cell_ID'][i] + '_R2.fq.gz') for i in range(0,len(cells['R2']))])
### List of both R1 and R2
proj_fastqs = list(cells['R1_projpath']) + list(cells['R2_projpath'])
trimmed_qc = [x.replace('.fq.gz', '_paired_fastqc.zip') for x in proj_fastqs]
### STAR bam file paths 
cells['bam'] = list([os.path.join('data', 'bam', cells.Cell_ID[i], cells.Cell_ID[i] + '_Aligned.sortedByCoord.out.bam') for i in range(0, len(cells['R1']))])
cells['flagstat'] = list([os.path.join('data', 'bam', cells.Cell_ID[i], 'Nreads.csv') for i in range(0, len(cells['R1']))])
### STAR count output paths
cells['tab'] = list([os.path.join('data', 'count', cells.Cell_ID[i], 'gene_abundances.tab') for i in range(0, len(cells['R1']))])
bam_files = cells['bam']
tab_files = cells['tab']

### Dictionary of flowcells, FullName:AbbreviatedName
#flowcells = dict(zip(cells.flowcell_full, cells.FCID))
### Dictionary whose values are lane regular expressions for each flowcell
#lane_regex = dict(zip(flowcells.keys(), ['s_[' + ''.join(map(str, list(set(cells[cells['FCID'] == fc].Lane)))) + ']'  for fc in list(flowcells.values())]))
### Dict whose values are lists of raw fastq files
#fc_fqs = dict(zip(flowcells.keys(), [list(cells[cells['FCID'] == fc].R1_fullpath) + list(cells[cells['FCID'] == fc].R2_fullpath) for fc in list(flowcells.values())]))
#proj_fqs = dict(zip(flowcells.keys(), [list(cells[cells['FCID'] == fc].R1) + list(cells[cells['FCID'] == fc].R2) for fc in list(flowcells.values())]))

### To define the fastqc zip files that will be requested
cells['R1_raw_qc'] = list([os.path.join('data', 'fastq', 'raw', cells['Cell_ID'][i], cells['R1'][i] + '_fastqc.zip') for i in range(0,len(cells['R1']))])
cells['R2_raw_qc'] = list([os.path.join('data', 'fastq', 'raw', cells['Cell_ID'][i], cells['R2'][i] + '_fastqc.zip') for i in range(0,len(cells['R2']))])
raw_qc = list(cells['R1_raw_qc']) + list(cells['R2_raw_qc'])
### From those request, get the raw fastq file for fastqc input
#get_raw_fastq = dict(zip(raw_fastq_names, runs_fastqs))
get_raw_fastq = dict(zip(runs_fastqs_key, runs_fastqs))
### Need dictionary Cell_ID -> R1 and Cell_ID -> R2

def get_raw_fastq_Rx(wildcards):
  return get_raw_fastq[wildcards.cell_id_R]

### Cell_ID -> raw fastq full path
get_raw_fastq1 = dict(zip(cells['Cell_ID'], cells['R1_fullpath']))
get_raw_fastq2 = dict(zip(cells['Cell_ID'], cells['R2_fullpath']))
def get_raw_fastq(wildcards):
  out = {'R1':get_raw_fastq1[wildcards.cell_id], 'R2':get_raw_fastq2[wildcards.cell_id]}
  return out

def get_raw_fastq_temp(wildcards):
  R1 = 'data/fastq/raw/' + wildcards.cell_id + '/R1.fq'
  R2 = 'data/fastq/raw/' + wildcards.cell_id + '/R2.fq'
  return {'R1':R1, 'R2':R2}

#cells['R1_gunzip'] = [x.replace('.gz', '') for x in cells['R1_fullpath']]
#cells['R2_gunzip'] = [x.replace('.gz', '') for x in cells['R2_fullpath']]
#get_gunzip_fastq1 = dict(zip(cells['Cell_ID'], cells['R1_gunzip']))
#get_gunzip_fastq2 = dict(zip(cells['Cell_ID'], cells['R2_gunzip']))
#print(cells['R1_gunzip'][1])
#def get_raw_fastq_temp(wildcards): 
#  #out = {'R1':get_raw_fastq1[wildcards.cell_id].replace('.gz', ''), 'R2':get_raw_fastq2[wildcards.cell_id].replace('.gz', '')}
#  out = {'R1':get_gunzip_fastq1[wildcards.cell_id], 'R2':get_gunzip_fastq2[wildcards.cell_id]}
#  return out


#trimmed_qc = [x.replace('/raw/', '/trimmed/') for x in raw_qc]
#get_trimmed_fastq = dict(zip(_fastq_names, runs_fastqs))
#def get_trimmed_fastq_Rx(wildcards):
#  return get_trimmed_fastq[wildcards.sample]

#get_fastq = dict(zip(fastq_names, runs_fastqs))
#def get_fastq_Rx(wildcards):
#  return get_fastq[wildcards.sample]

QC_name = config['QC_name']
results_dir = os.path.join('results', QC_name) + '/'
print(results_dir)

### Read QC summary file (input to batch_eval checkpoint) and creates dictionary to hold the file paths held within
QC_file = os.path.join(os.getcwd(), config['QC_file'])
qcdat = pd.read_csv(QC_file)
### Add missing steps to qcdat with '' file column, including step 0 (no QC done yet)
steps = list(set(list(range(6))) - set(qcdat.step))
d = {'step':steps, 'file':['' for s in steps]}
qcdat = pd.concat([qcdat, pd.DataFrame(d)])
### Reorder rows
qcdat['step_num'] = ['step' + str(q) for q in qcdat.step]
qcdat['post_name'] = [results_dir + 'PostQC' + str(q) + '.RDS' for q in qcdat.step]
qcdat = qcdat.set_index('step')
qcdat = qcdat.sort_index(ascending = True)
### Add step_name
qcdat['step_name'] = ['Base data', 'Sample filter', 'Imputation', 'Cell filter', 'Integration', 'Cluster filter']
### Add original RDS file
qcdat.iloc[0, qcdat.columns.get_loc('post_name')] = 'data/All_data.RDS'
### Add skip information
qcdat['skip'] = [f == '' for f in qcdat.file]
qcdat.iloc[0, qcdat.columns.get_loc('skip')] = False
#qcdat.to_csv(results_dir + 'QC_steps.csv', sep = ',')

QC_specs = defaultdict(str, zip(qcdat.step_num, qcdat.file))
print(qcdat)
print(qcdat.iloc[4,0])

### If the last line of the second output of cluster_filters (Filters.txt) specifies that 0 cells were filtered, skip re-clustering
def maybe_skip_second_cluster(wildcards):
  checkp = checkpoints.cluster_filter.get().output[1]
  with open(checkp) as f:
    for line in f:
      if(re.search('total filtered', line)):
        filtered_line = line
  print(filtered_line)
  n_filtered = filtered_line.split(',')[-1] ### Last item in the line is n
  if(int(n_filtered) == 0):
    print('Skip second cluster.')
    return results_dir + 'Cluster_filtered.RDS'
  else:
    print('Do second cluster.')
    return results_dir + 'Clustered_again.RDS'

def maybe_skip_QC_stepN(qcdat, stepN, spec = True):
  ### txt output from batch_evaluations rule
  #qc = pd.read_csv(qc_csv)
  QC_files = defaultdict(str, zip(qcdat.step_num, qcdat.file))
  QC_rds = dict(zip(qcdat.step_num, qcdat.post_name))
  specs_file = QC_files['step' + str(stepN)]
  ### Slice the data frame to include only up to input stepN
  qcdat = qcdat[:stepN]
  ### Select the step name of the last step that is not skipped (column 'skip' = F). Use that step name as key in the RDS dictionary.
  rds_file = QC_rds[qcdat[qcdat.skip == False].iloc[-1, qcdat.columns.get_loc('step_num')]]
  out = {'rds':rds_file, 'touch':'data/checkpoint.done'}
  if spec:
    out['qc'] = specs_file
  return(out)


#def maybe_skip_annotation(species, tissue):
#  if(species.lower() in ['hsapiens', 'human', 'homo sapiens', 'h. sapiens'] and tissue.lower() in ['pbmc', 't']):
def maybe_skip_annotation(annotation):
  if(annotation != ''):
    out = results_dir + 'Mapped.RDS'
    print('Doing annotation')
  else:
    out = maybe_skip_second_cluster
  return out

include: "/hpcdata/vrc/vrc1_data/douek_lab/snakemakes/SingleCell/Snakefile"

#module general_SC_analysis:
#  snakefile:
#    "/hpcdata/vrc/vrc1_data/douek_lab/snakemakes/SmartSeq/SC_R_subset.py"
#use rule * from general_SC_analysis

rule all:
  input: 
    results_dir + 'App/Data.RData',
    results_dir + 'Cluster_DE.xls',
    [results_dir +  "DE/{test}_GSEA.xls".format(test = test) for test in tests]
#    'data/multiqcs.txt'

### Pipeline starts after bcl2fastq has been completed separately
rule fastqc_raw_once:
  input:
    unpack(get_raw_fastq)
  output:
    'data/fastq/raw/{cell_id}/read1_{raw_name}_fastqc.zip',
    'data/fastq/raw/{cell_id}/read2_{raw_name}_fastqc.zip'
  shell:
    """
    fastqc --noextract -o data/fastq/raw/{wildcards.cell_id}/ {input.R1}
    fastqc --noextract -o data/fastq/raw/{wildcards.cell_id}/ {input.R2}
    """

rule fastqc_trimmed_once:
  input:
    'data/fastq/trimmed/{cell_id}/{sample}_paired.fq.gz'
  output:
    'data/fastq/trimmed/{cell_id}/{sample}_paired_fastqc.zip'
  shell:
    "fastqc --noextract -o data/fastq/trimmed/{wildcards.cell_id}/ {input}"

### wildcard sdir cannot go in the expand function with the non-wildcard fastq
rule aggregate_fastqc_raw:
  input:
    raw_qc
  output:
    "data/fastq/raw/multiqc_input_files.txt"
  shell:
    "ls {input} > {output}"

rule aggregate_fastqc_trimmed:
  input:
    trimmed_qc
  output:
    "data/fastq/trimmed/multiqc_input_files.txt"
  shell:
    "ls {input} > {output}"

rule aggregate_bams:
  input:
    expand('data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam.bai', cell_id = cells['Cell_ID'])
  output:
    'data/bam/bam_files.txt'
  shell:
    'ls {input} > {output}'

rule aggregate_rseqc:
  input:
    expand('data/bam/{cell_id}/RSeQC/multiqc_input_files.txt', cell_id = cells['Cell_ID'])
  output:
    'data/bam/multiqc_input_files.txt'
  shell:
    "cat {input} > {output}"

rule aggregate_trim_stderr:
  input: 
    trimerr_files
  output:
    "data/trims.txt"
  shell:
    "ls {input} > {output}"

rule aggregate_multiqc:
  input:
    raw_fastq = 'data/fastq/raw/multiqc.html',
    trimmed_fastq = 'data/fastq/trimmed/multiqc.html',
    bam = 'data/bam/multiqc.html'
  output:
    'data/multiqcs.txt'
  shell:
    'ls {input} > {output}'

rule multiqc:
  input:
    "data/{sdir}/multiqc_input_files.txt"
  resources:mem_mb=240000
  output:
    "data/{sdir}/multiqc.html"
  shell:
    "multiqc --file-list {input} -f --filename {output}"

rule gunzip_once:
  input:
    unpack(get_raw_fastq)
  output:
    R1 = temp('data/fastq/raw/{cell_id}/R1.fq'),
    R2 = temp('data/fastq/raw/{cell_id}/R2.fq')
  shell:
    """
    gunzip -c {input.R1} > {output.R1}
    gunzip -c {input.R2} > {output.R2}
    """

rule trimmomatic_once:
  input:
    unpack(get_raw_fastq_temp)
  params:
    adapter_file = adapter_file
  output:
    R1_paired = "data/fastq/trimmed/{cell_id}/{cell_id}_R1_paired.fq.gz",
    R1_unpaired = "data/fastq/trimmed/{cell_id}/{cell_id}_R1_unpaired.fq.gz",
    R2_paired = "data/fastq/trimmed/{cell_id}/{cell_id}_R2_paired.fq.gz",
    R2_unpaired = "data/fastq/trimmed/{cell_id}/{cell_id}_R2_unpaired.fq.gz",
  conda:
    "/hpcdata/vrc/vrc1_data/douek_lab/snakemakes/RRBSeq/py2.7.yml"
  log:
    log1 = "data/fastq/trimmed/{cell_id}/trim_stdout.txt",
    log2 = "data/fastq/trimmed/{cell_id}/trim_stderr.txt",
  shell:
    """
    java -Xmx7G -jar /hpcdata/vrc/vrc1_data/douek_lab/programs/Trimmomatic-0.39/trimmomatic-0.39.jar PE -phred33 -threads 1 {input.R1} {input.R2} {output.R1_paired} {output.R1_unpaired} {output.R2_paired} {output.R2_unpaired} ILLUMINACLIP:{params.adapter_file}:2:30:10:4:true LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:50 1>{log.log1} 2>{log.log2}
    """

rule STAR_align_once:
  input: 
    "data/fastq/trimmed/{cell_id}/{cell_id}_R1_paired.fq.gz",
    "data/fastq/trimmed/{cell_id}/{cell_id}_R2_paired.fq.gz"
  params:
    starDB = config['starDB']
  output:
    bam = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam",
    starlog = 'data/bam/{cell_id}/{cell_id}_Log.final.out'
  resources: mem_mb=30000
  log:
    outlog = "data/bam/{cell_id}/logs/STAR.olog",
    errlog = "data/bam/{cell_id}/logs/STAR.elog"
  shell:
    """
    mkdir -p data/fastq/bam/{wildcards.cell_id}/RSeQC/
    mkdir -p data/fastq/bam/{wildcards.cell_id}/logs/
    STAR --genomeDir {params.starDB} --readFilesIn {input} --outFileNamePrefix data/bam/{wildcards.cell_id}/{wildcards.cell_id}_ --readFilesCommand gzip -dc --outReadsUnmapped Fastx --outSAMtype BAM SortedByCoordinate --outFilterMultimapNmax 20 --outFilterMismatchNoverReadLmax 0.04 --limitSjdbInsertNsj 1200000 1>{log.outlog} 2>{log.errlog}
   """

rule flagstat_once:
  input:
    "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam"
  output:
    "data/bam/{cell_id}/Nreads.csv"
  shell:
    """
    echo -n {wildcards.cell_id}, >> {output}
    samtools flagstat {input} | grep 'in total' | awk 'BEGIN{{FS=" "}} {{print $1}}' | xargs echo >> {output}
    """
    
rule aggr_flagstat:
  input:
    cells['flagstat']
  output:
    "data/bam/Nreads.csv"
  shell:
    "cat {input} > {output}"

checkpoint filter_empty:
  input:
    "data/bam/Nreads.csv"
  params:
    scripts = config['SmartSeq_scripts']
  output:
    "data/Post_filter.csv"
  shell:
    "Rscript {params.scripts}/Empty_cell_filter.R {input} {output}"

rule index_bam_once:
  input:
    "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam",
  log:
    outlog = "data/bam/{cell_id}/logs/index.olog",
    errlog = "data/bam/{cell_id}/logs/index.elog"
  output:
    "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam.bai",
  shell:
    "samtools index {input} 1>{log.outlog} 2>{log.errlog}"

rule idxstats_once:
  input:
    "data/bam/{cell_id}}/{cell_id}_Aligned.sortedByCoord.out.bam",
  output:
    "data/bam/{cell_id}/Star_alignment_stats.txt"
  shell:
    "samtools idxstats {input} > {output}"

###  These two RSeQC functions take so long that they will run separately
rule RSeQC_tin:
  input:
    bam = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam",
    bai = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam.bai"
  params:
    anno = config['refbed'],
  output: 
    tin = 'data/bam/{cell_id}/RSeQC/{cell_id}.out.summary.txt',
    xls = 'data/bam/{cell_id}/RSeQC/{cell_id}.out.tin.xls'
  shell:
    """
    mkdir -p data/bam/{wildcards.cell_id}/RSeQC/
    tin.py -i {input.bam} -r {params.anno} > {output.tin}
    """

rule RSeQC_gbc:
  input: 
    bam = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam",
    bai = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam.bai"
  params:
    anno = config['refbed']
  output:
    geneBody_cov = 'data/bam/{cell_id}/RSeQC/{cell_id}.geneBodyCoverage.txt'
  shell:
    """
    mkdir -p data/bam/{wildcards.cell_id}/RSeQC/
    geneBody_coverage.py -i {input.bam} -o data/bam/{wildcards.cell_id}/RSeQC/{wildcards.cell_id} -r {params.anno}
    """

rule RSeQC_once:
  input:
    bam = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam",
    bai = "data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam.bai"
  params:
    anno = config['refbed'],
    out_pref = 'data/bam/{cell_id}/RSeQC/{cell_id}'
  output:
    bam_stat = 'data/bam/{cell_id}/RSeQC/{cell_id}.bam_stat.txt',
    read_dist = 'data/bam/{cell_id}/RSeQC/{cell_id}.read_distribution.txt',
    infer_exp = 'data/bam/{cell_id}/RSeQC/{cell_id}.infer_experiment.txt',
    inner_dist = 'data/bam/{cell_id}/RSeQC/{cell_id}.inner_distance_freq.txt',
    junction_anno = 'data/bam/{cell_id}/RSeQC/{cell_id}_junction_annotation.txt',
    junction_sat = 'data/bam/{cell_id}/RSeQC/{cell_id}.junctionSaturation_plot.r',
    read_GC = 'data/bam/{cell_id}/RSeQC/{cell_id}.GC.xls',
    dup_rate = 'data/bam/{cell_id}/RSeQC/{cell_id}.pos.DupRate.xls',
    RNA_frag_size = 'data/bam/{cell_id}/RSeQC/{cell_id}.RNA_fragment_size.txt'
  shell:
    """
    mkdir -p data/bam/{wildcards.cell_id}/RSeQC/
    bam_stat.py -i {input.bam} > {output.bam_stat}
    read_distribution.py -i {input.bam} -r {params.anno} > {output.read_dist}
    infer_experiment.py -i {input.bam} -r {params.anno} > {output.infer_exp}
    inner_distance.py -i {input.bam} -o {params.out_pref} -r {params.anno}
    junction_annotation.py -i {input.bam} -o {params.out_pref} -r {params.anno} 2>{output.junction_anno}
    junction_saturation.py -i {input.bam} -o {params.out_pref} -r {params.anno}
    read_GC.py -i {input.bam} -o {params.out_pref}
    read_duplication.py -i {input.bam} -o {params.out_pref}
    #read_NVC.py -i {input} -o {params.out_pref}
    #stread_quality.py -i {input} -o {params.out_pref}
    RNA_fragment_size.py -i {input} -r {params.anno} > {output.RNA_frag_size}
    #RPKM_saturation.py -i {input} -o {params.out_pref} -r {params.anno}
    #mismatch_profile.py -i {input} -o {params.out_pref}
    """

rule aggregate_RSeQC:
  input:
    bam_stat = 'data/bam/{cell_id}/RSeQC/{cell_id}.bam_stat.txt',
    read_dist = 'data/bam/{cell_id}/RSeQC/{cell_id}.read_distribution.txt',
    infer_exp = 'data/bam/{cell_id}/RSeQC/{cell_id}.infer_experiment.txt',
    inner_dist = 'data/bam/{cell_id}/RSeQC/{cell_id}.inner_distance_freq.txt',
    junction_anno = 'data/bam/{cell_id}/RSeQC/{cell_id}_junction_annotation.txt',
    junction_sat = 'data/bam/{cell_id}/RSeQC/{cell_id}.junctionSaturation_plot.r',
    read_GC = 'data/bam/{cell_id}/RSeQC/{cell_id}.GC.xls',
    dup_rate = 'data/bam/{cell_id}/RSeQC/{cell_id}.pos.DupRate.xls',
    RNA_frag_size = 'data/bam/{cell_id}/RSeQC/{cell_id}.RNA_fragment_size.txt',
    geneBody_cov = 'data/bam/{cell_id}/RSeQC/{cell_id}.geneBodyCoverage.txt',
    tin = 'data/bam/{cell_id}/RSeQC/{cell_id}.out.summary.txt',
    starlog = 'data/bam/{cell_id}/{cell_id}_Log.final.out'
  output:
    'data/bam/{cell_id}/RSeQC/multiqc_input_files.txt'
  shell:
    'ls {input} > {output}'

rule stringtie_once:
  input:
    bam = 'data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam',
    bai = 'data/bam/{cell_id}/{cell_id}_Aligned.sortedByCoord.out.bam.bai'
  output:
    tab = 'data/count/{cell_id}/gene_abundances.tab'
  log:
    gtflog = 'data/count/{cell_id}/stringtie.gtf',
    txtlog = 'data/count/{cell_id}/stringtie_stderr.txt'
  params:
    ref = config['refgtf']
  #resources:mem_mb=240000
  shell:
    'stringtie {input.bam} -G {params.ref} -A {output.tab} -e 1> {log.gtflog} 2> {log.txtlog}'

### From checkpoint 'filter_empty'
def get_filtered_cells_as_tab_files(wildcards):
  checkpoint_output = checkpoints.filter_empty.get(**wildcards).output[0]
  #checkpoint_output = checkpoints.filter_empty
  ### read it to get list of cell IDs
  cells_filt = pd.read_table(checkpoint_output, sep = ',').set_index("ID", drop = False)
  ### Add 'data/count/{cell_id}/gene_abundances.tab' to each
  cells_filt['tabs'] = 'data/count/' + cells_filt['ID'].map(str) + '/gene_abundances.tab'
  ### return input to create_mtx, list of tab files per cell 
  return list(cells_filt['tabs'])

rule create_mtx:
  input:
    #config['tabs']
    get_filtered_cells_as_tab_files ### checks results from checkpoint
  params:
    project = config['project'],
    Cell_sheet = config['Cell_sheet'],
    scripts = config['SmartSeq_scripts']
  output:
    mtx_file = 'data/raw_mtx.RDS'
  shell: ### Currently this determines tab files from those that exist from Cell_sheet. It could instead take input directly here.
    "Rscript {params.scripts}/tab2mtx.R {params.project} {params.Cell_sheet} {output}"

### Filters cells based on general values. Should be updated to take inputs (as CITESeq pipepline)
rule compile_QC_metrics:
  input:
    ancient('data/raw_mtx.RDS'),
    'data/gtf.RDS'
  params:
    runs_dir = config['runs_dir'],
    project = config['project'],
    Cell_sheet = config['Cell_sheet'],
    demult = demult,
    scripts = config['SmartSeq_scripts']
  output:
    'data/Covariates_QC_metrics.csv'
    #'data/Cell_filters.txt',
    #'data/Cell_filters.pdf'
  shell:
    'Rscript {params.scripts}/SC_Compile_QC_metrics.R {input} {params.runs_dir} {params.project} {params.Cell_sheet} {params.demult} {output}'

rule create_seurat_object:
  input:
    ancient('data/raw_mtx.RDS'),
    'data/Covariates_QC_metrics.csv',
    'data/gtf.RDS'
  params:
    project = config['project'],
    scripts = config['SmartSeq_scripts']
  output:
    'data/All_data.RDS'
  shell:
    'Rscript {params.scripts}/mtx2seurat.R {params.project} {input} {output}'

