import pandas as pd
import subprocess
import os
import re
from collections import defaultdict
import csv

configfile: 'project_config_file.yaml'
localrules: aggregate_fastqc_raw, aggregate_trim_stderr, aggregate_RSeQC
### Read configuration file
config = defaultdict(str, config)

#### Assume demultiplexing was done with bcl2fastq2 with specified adapters in sample sheet.
demult = True
if config['demultiplex'] == 'SSSS':
  demult = False ### Unless 'SSSS' is input, meaning Sam's sans-sample-sheet demultiplexing code
if config['adapter'] == 'Illumina':
  adapter_file = '/hpcdata/vrc/vrc1_data/douek_lab/programs/Trimmomatic-0.39/adapters/TruSeq3-PE.fa'
elif config['adapter'] == 'Nextera':
  adapter_file = '/hpcdata/vrc/vrc1_data/douek_lab/programs/Trimmomatic-0.39/adapters/NexteraPE-PE.fa'

do_annot = False
if(config['annotation_reference'] == ''):
  do_annot = True

cell_sheet = os.path.join(os.getcwd(), config['Cell_sheet'])
cells = pd.read_table(cell_sheet, sep = ',').set_index("Cell_ID", drop = False)
print('Size of input sample sheet')
print(cells.shape)

cells['Lane'] = cells['Lane'].apply(str)
### Specific to Kristins T, bcl2fastq won't produce results for one sample
#cells = cells[(cells['flowcell_full'] != '210401_K00241_0189_BHKMVMBBXY') & (cells['Lane'] != '8')]
#cells = cells[(cells['flowcell_full'] != '210401_K00241_0189_BHKMVMBBXY')]

if 'clust_resolution2' not in config.keys():
  config['clust_resolution2'] = config['cluster_resolution']

comp_dict = defaultdict(int)
with open(config['comparison_file'], newline = '') as csvfile:
  reader = csv.DictReader(csvfile)
  keyint = 0
  for row in reader:
    comp_dict[keyint] = row
    ### If no input strat_name, set dictionary values to 'All'
    if(comp_dict[keyint]['strat_name1'] == ''):
      comp_dict[keyint]['strat_name1'] = 'All'
      comp_dict[keyint]['strat_values1A'] = 'All'
      comp_dict[keyint]['strat_values1B'] = 'All'
    else: ### If strat_name is input, and strat_valueA is input but strat_value B is not, set the second value to the first
      if(comp_dict[keyint]['strat_values1A'] != '' and comp_dict[keyint]['strat_values1B'] == ''):
        comp_dict[keyint]['strat_values1B'] = comp_dict[keyint]['strat_values1A']
    ### If no input strat_name, set dictionary values to 'All'
    if(comp_dict[keyint]['strat_name2'] == ''):
      comp_dict[keyint]['strat_name2'] = 'All'
      comp_dict[keyint]['strat_values2A'] = 'All'
      comp_dict[keyint]['strat_values2B'] = 'All'
    else: ### If strat_name is input, and strat_valueA is input but strat_value B is not, set the second value to the first
      if(comp_dict[keyint]['strat_values2A'] != '' and comp_dict[keyint]['strat_values2B'] == ''):
        comp_dict[keyint]['strat_values2B'] = comp_dict[keyint]['strat_values2A']
    ### If either test value is empty, set to NA
    ### If either test value is empty, set to NA
    if(comp_dict[keyint]['value1'] == ''): 
      comp_dict[keyint]['value1'] = 'NA'
    if(comp_dict[keyint]['value2'] == ''): 
      comp_dict[keyint]['value2'] = 'NA'
    keyint += 1
print(comp_dict)

### Read list of clusters to sub-cluster
sc_file = os.path.join(os.getcwd(), config['sub_cluster'])
if os.path.exists(sc_file):
  sc = pd.read_csv(sc_file)
  ### Because cluster names are likely numbers only
  sc['cluster'] = sc['cluster'].astype(str)
sub_clusters = ['-'.join(list(row)) for index,row in sc.iterrows()]
print('Sub clustering: ')
print(sub_clusters)

if demult: ### Requires Cell_ID, Cell_Name, S_N, Lane and flowcell_full
  ### fastq file names  output from bcl2fastq2
  cells['R1'] = cells['Cell_Name'].map(str) + '_' + cells['S_N'].map(str) + '_L00' + cells['Lane'].map(str) + '_R1_001' 
  cells['R2'] = cells['Cell_Name'].map(str) + '_' + cells['S_N'].map(str) + '_L00' + cells['Lane'].map(str) + '_R2_001' 
  ### fastq file paths  output from bcl2fastq (and then Sam's demultiplexing?)
  #cells['R1_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'bcl2fastq2', config['project'], cells['Cell_ID'][i], cells['R1'][i] + '.fastq.gz') for i in range(0, len(cells['R1']))])
  #cells['R2_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'bcl2fastq2', config['project'], cells['Cell_ID'][i], cells['R2'][i] + '.fastq.gz') for i in range(0,len(cells['R1']))])
  cells['R1_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', config['project'], cells['Cell_ID'][i], cells['R1'][i] + '.fastq.gz') for i in range(0, len(cells['R1']))])
  cells['R2_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', config['project'], cells['Cell_ID'][i], cells['R2'][i] + '.fastq.gz') for i in range(0,len(cells['R1']))])
else:  ### Requires UDP_ID and Lane and flowcell_full
  ### fastq file names  output from Sam's sans-sample-sheet code
  cells['R1'] = 'read1_index_' + cells['UDP_ID'].map(str) 
  cells['R2'] = 'read2_index_' + cells['UDP_ID'].map(str)
  #cells['R1'] = 'paired_read1_index_' + cells['UDP_ID'].map(str) 
  #cells['R2'] = 'paired_read2_index_' + cells['UDP_ID'].map(str)
  ### fastq file paths  output from bcl2fastq (and then Sam's demultiplexing?)
  cells['R1_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', cells['Lane'][i], cells['UDP_ID'][i], cells['R1'][i] + '.fq.gz') for i in range(0, len(cells['R1']))])
  cells['R2_fullpath'] = list([os.path.join(config['runs_dir'], cells.flowcell_full[i], 'demultiplexed', cells['Lane'][i], cells['UDP_ID'][i], cells['R2'][i] + '.fq.gz') for i in range(0,len(cells['R1']))])

print('Before and after filtering for existing but empty fastq.gz files')
print(cells.shape)

### Remove cells with raw fastq files that do not exist
cells['filt'] = list([os.path.exists(x) for x in cells['R1_fullpath']])
print(str(sum(cells['filt'] == False)) + ' removed because fastq.gz do not exist')

cells = cells[cells['filt'] == True]
### Remove cells with raw fastq files that exist but are empty
cells['filt'] = list([(os.path.getsize(x) > 0) for x in cells['R1_fullpath']])
print(str(sum(cells['filt'] == False)) + ' removed because fastq.gz have no size')
cells = cells[cells['filt'] == True]

print(cells.shape)

### List of full paths to R1 and R2
runs_fastqs = list(cells['R1_fullpath']) + list(cells['R2_fullpath'])
runs_fastqs_key = list([cid + '_R1' for cid in cells['Cell_ID']]) + list([cid + '_R2' for cid in cells['Cell_ID']])
### Need dictionary Cell_ID -> R1 and Cell_ID -> R2

#cells['Cell_ID_R1'] = cells['Cell_ID'].map(str) + '_R1'
#cells['Cell_ID_R2'] = cells['Cell_ID'].map(str) + '_R2'
#fastq_names = list(cells['Cell_ID_R1']) + list(cells['Cell_ID_R2'])
raw_fastq_names = list(cells['R1']) + list(cells['R2'])


### Trimmomatic error logs per cell
cells['trimerr'] = list([os.path.join('data', 'fastq', 'trimmed', cells['Cell_ID'][i], 'trim_stderr.txt') for i in range(0,len(cells['R1']))])
trimerr_files = cells['trimerr']
### fastq file paths output from trimmomatic and moved to project directory. IS THIS CURRENTLY USED?
cells['R1_projpath'] = list([os.path.join('data', 'fastq', 'trimmed', cells['Cell_ID'][i], cells['Cell_ID'][i] + '_R1.fq.gz') for i in range(0,len(cells['R1']))])
cells['R2_projpath'] = list([os.path.join('data', 'fastq', 'trimmed', cells['Cell_ID'][i], cells['Cell_ID'][i] + '_R2.fq.gz') for i in range(0,len(cells['R2']))])
### List of both R1 and R2
proj_fastqs = list(cells['R1_projpath']) + list(cells['R2_projpath'])
trimmed_qc = [x.replace('.fq.gz', '_paired_fastqc.zip') for x in proj_fastqs]
### STAR bam file paths 
cells['bam'] = list([os.path.join('data', 'bam', cells.Cell_ID[i], cells.Cell_ID[i] + '_Aligned.sortedByCoord.out.bam') for i in range(0, len(cells['R1']))])
cells['flagstat'] = list([os.path.join('data', 'bam', cells.Cell_ID[i], 'Nreads.csv') for i in range(0, len(cells['R1']))])
### STAR count output paths
cells['tab'] = list([os.path.join('data', 'count', cells.Cell_ID[i], 'gene_abundances.tab') for i in range(0, len(cells['R1']))])
bam_files = cells['bam']
tab_files = cells['tab']

### Dictionary of flowcells, FullName:AbbreviatedName
#flowcells = dict(zip(cells.flowcell_full, cells.FCID))
### Dictionary whose values are lane regular expressions for each flowcell
#lane_regex = dict(zip(flowcells.keys(), ['s_[' + ''.join(map(str, list(set(cells[cells['FCID'] == fc].Lane)))) + ']'  for fc in list(flowcells.values())]))
### Dict whose values are lists of raw fastq files
#fc_fqs = dict(zip(flowcells.keys(), [list(cells[cells['FCID'] == fc].R1_fullpath) + list(cells[cells['FCID'] == fc].R2_fullpath) for fc in list(flowcells.values())]))
#proj_fqs = dict(zip(flowcells.keys(), [list(cells[cells['FCID'] == fc].R1) + list(cells[cells['FCID'] == fc].R2) for fc in list(flowcells.values())]))

### To define the fastqc zip files that will be requested
cells['R1_raw_qc'] = list([os.path.join('data', 'fastq', 'raw', cells['Cell_ID'][i], cells['R1'][i] + '_fastqc.zip') for i in range(0,len(cells['R1']))])
cells['R2_raw_qc'] = list([os.path.join('data', 'fastq', 'raw', cells['Cell_ID'][i], cells['R2'][i] + '_fastqc.zip') for i in range(0,len(cells['R2']))])
raw_qc = list(cells['R1_raw_qc']) + list(cells['R2_raw_qc'])
### From those request, get the raw fastq file for fastqc input
#get_raw_fastq = dict(zip(raw_fastq_names, runs_fastqs))
get_raw_fastq = dict(zip(runs_fastqs_key, runs_fastqs))
### Need dictionary Cell_ID -> R1 and Cell_ID -> R2

def get_raw_fastq_Rx(wildcards):
  return get_raw_fastq[wildcards.cell_id_R]

### Cell_ID -> raw fastq full path
get_raw_fastq1 = dict(zip(cells['Cell_ID'], cells['R1_fullpath']))
get_raw_fastq2 = dict(zip(cells['Cell_ID'], cells['R2_fullpath']))
def get_raw_fastq(wildcards):
  out = {'R1':get_raw_fastq1[wildcards.cell_id], 'R2':get_raw_fastq2[wildcards.cell_id]}
  return out

def get_raw_fastq_temp(wildcards):
  R1 = 'data/fastq/raw/' + wildcards.cell_id + '/R1.fq'
  R2 = 'data/fastq/raw/' + wildcards.cell_id + '/R2.fq'
  return {'R1':R1, 'R2':R2}

#cells['R1_gunzip'] = [x.replace('.gz', '') for x in cells['R1_fullpath']]
#cells['R2_gunzip'] = [x.replace('.gz', '') for x in cells['R2_fullpath']]
#get_gunzip_fastq1 = dict(zip(cells['Cell_ID'], cells['R1_gunzip']))
#get_gunzip_fastq2 = dict(zip(cells['Cell_ID'], cells['R2_gunzip']))
#print(cells['R1_gunzip'][1])
#def get_raw_fastq_temp(wildcards): 
#  #out = {'R1':get_raw_fastq1[wildcards.cell_id].replace('.gz', ''), 'R2':get_raw_fastq2[wildcards.cell_id].replace('.gz', '')}
#  out = {'R1':get_gunzip_fastq1[wildcards.cell_id], 'R2':get_gunzip_fastq2[wildcards.cell_id]}
#  return out


#trimmed_qc = [x.replace('/raw/', '/trimmed/') for x in raw_qc]
#get_trimmed_fastq = dict(zip(_fastq_names, runs_fastqs))
#def get_trimmed_fastq_Rx(wildcards):
#  return get_trimmed_fastq[wildcards.sample]

#get_fastq = dict(zip(fastq_names, runs_fastqs))
#def get_fastq_Rx(wildcards):
#  return get_fastq[wildcards.sample]

QC_name = config['QC_name']
results_dir = os.path.join('results', QC_name) + '/'
print(results_dir)

### Read QC summary file (input to batch_eval checkpoint) and creates dictionary to hold the file paths held within
QC_file = os.path.join(os.getcwd(), config['QC_file'])
qcdat = pd.read_csv(QC_file)
### Add missing steps to qcdat with '' file column, including step 0 (no QC done yet)
steps = list(set(list(range(6))) - set(qcdat.step))
d = {'step':steps, 'file':['' for s in steps]}
qcdat = pd.concat([qcdat, pd.DataFrame(d)])
### Reorder rows
qcdat['step_num'] = ['step' + str(q) for q in qcdat.step]
qcdat['post_name'] = [results_dir + 'PostQC' + str(q) + '.RDS' for q in qcdat.step]
qcdat = qcdat.set_index('step')
qcdat = qcdat.sort_index(ascending = True)
### Add step_name
qcdat['step_name'] = ['Base data', 'Sample filter', 'Imputation', 'Cell filter', 'Integration', 'Cluster filter']
### Add original RDS file
qcdat.iloc[0, qcdat.columns.get_loc('post_name')] = 'data/All_data.RDS'
### Add skip information
qcdat['skip'] = [f == '' for f in qcdat.file]
qcdat.iloc[0, qcdat.columns.get_loc('skip')] = False
#qcdat.to_csv(results_dir + 'QC_steps.csv', sep = ',')

QC_specs = defaultdict(str, zip(qcdat.step_num, qcdat.file))
print(qcdat)
print(qcdat.iloc[4,0])

### If the last line of the second output of cluster_filters (Filters.txt) specifies that 0 cells were filtered, skip re-clustering
def maybe_skip_second_cluster(wildcards):
  checkp = checkpoints.cluster_filter.get().output[1]
  with open(checkp) as f:
    for line in f:
      if(re.search('total filtered', line)):
        filtered_line = line
  print(filtered_line)
  n_filtered = filtered_line.split(',')[-1] ### Last item in the line is n
  if(int(n_filtered) == 0):
    print('Skip second cluster.')
    return results_dir + 'Cluster_filtered.RDS'
  else:
    print('Do second cluster.')
    return results_dir + 'Clustered_again.RDS'

def maybe_skip_QC_stepN(qcdat, stepN, spec = True):
  ### txt output from batch_evaluations rule
  #qc = pd.read_csv(qc_csv)
  QC_files = defaultdict(str, zip(qcdat.step_num, qcdat.file))
  QC_rds = dict(zip(qcdat.step_num, qcdat.post_name))
  specs_file = QC_files['step' + str(stepN)]
  ### Slice the data frame to include only up to input stepN
  qcdat = qcdat[:stepN]
  ### Select the step name of the last step that is not skipped (column 'skip' = F). Use that step name as key in the RDS dictionary.
  rds_file = QC_rds[qcdat[qcdat.skip == False].iloc[-1, qcdat.columns.get_loc('step_num')]]
  out = {'rds':rds_file, 'touch':'data/checkpoint.done'}
  if spec:
    out['qc'] = specs_file
  return(out)


def maybe_skip_annotation(annotation):
  if(annotation != ''):
    out = results_dir + 'Mapped.RDS'
    print('Doing annotation')
  else:
    out = maybe_skip_second_cluster
  return out

include: "/hpcdata/vrc/vrc1_data/douek_lab/snakemakes/SingleCell/Snakefile"

#module general_SC_analysis:
#  snakefile:
#    "/hpcdata/vrc/vrc1_data/douek_lab/snakemakes/SmartSeq/SC_R_subset.py"
#use rule * from general_SC_analysis

rule all:
  input: 
    results_dir + 'App/Data.RData',
    results_dir + 'Cluster_DE.xls',
    results_dir +  "DE/GSEA.xls"
#    'data/multiqcs.txt'

